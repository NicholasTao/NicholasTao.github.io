## 为什么做新框架？

### 新框架目标

1. 大规模性能提高至10k以上							原因: 优化了升级框架内部瓶颈点
2. 现网Bug减少至1/2										原因: 代码可信整改
3. 升级框架新需求开发工作量减少至1/3		原因: 代码架构优化
4. 周边组件测试时间缩短至1/4						原因: 新特性，细粒度上下文无关的升级回退

## The Header

[[[Link to Header](https://github.com/NicholasTao/NicholasTao.github.io/issues/8#the-header)](https://github.com/NicholasTao/NicholasTao.github.io/issues/8#the-header)](#the-header)

### 新增的特性

1. 灵活升级
   细粒度任务独立回退升级, 任意小工步升级失败后可以独立回退。并重试或添加规避代码后重试

   单工步调试：开发过程中可以单工步调试，且无需依赖前后工步。如单独调试升级配置转换(这是一线人员重点要求添加的功能)

   详细设计: [[灵活升级特性设计]](#灵活升级特性设计)

   旧框架实现对比: [[旧框架设计]](#旧框架设计)

   影响: 开发中的测试效率, 生产中的规避效率

   

2. 并行升级
   工程一升级AB组件的同时，工程二可以并行升级无依赖的CD组件

   详细设计: [[[并行升级特性设计](https://github.com/NicholasTao/NicholasTao.github.io/issues/8#%E5%B9%B6%E8%A1%8C%E5%8D%87%E7%BA%A7%E7%89%B9%E6%80%A7%E8%AE%BE%E8%AE%A1)](https://github.com/NicholasTao/NicholasTao.github.io/issues/8#%E5%B9%B6%E8%A1%8C%E5%8D%87%E7%BA%A7%E7%89%B9%E6%80%A7%E8%AE%BE%E8%AE%A1)](#并行升级特性设计)

   旧框架实现对比: [[[[旧框架设计]](https://github.com/NicholasTao/NicholasTao.github.io/issues/8#%E6%97%A7%E6%A1%86%E6%9E%B6%E8%AE%BE%E8%AE%A1)](https://github.com/NicholasTao/NicholasTao.github.io/issues/8#%E6%97%A7%E6%A1%86%E6%9E%B6%E8%AE%BE%E8%AE%A1)](#旧框架设计)

   影响: 升级效率，升级灵活性

   

3. 多版本升级
   支持一个版本的组件升级到不同版本，如控制节点升级OS，计算节点不升级OS。控制节点升级到X版本Nova，计算节点升级到Y版本Nova

   [[[多版本升级特性设计]](https://github.com/NicholasTao/NicholasTao.github.io/issues/8#%E5%A4%9A%E7%89%88%E6%9C%AC%E5%8D%87%E7%BA%A7%E7%89%B9%E6%80%A7%E8%AE%BE%E8%AE%A1)](#多版本升级特性设计)

   

4. N-X升级
   支持N-X升级的升级框架支持
   a)	提供管理面备区升级能力，管理组件随hostos在备区升级后重启生效；同时保留管理面重启单独组件升级方式

   

5. 可视化升级(未来特性)
   升级前可以生成整个升级计划书，供用户和技术人员检视，并可以根据需要灵活修改。升级过程不再是盲盒

### 改善的特性

6. 并行化
   编排、任务下发、任务上报接收、任务执行、查询进度都可以用独立进程执行，其中任务下发、任务上报接收、任务执行、查询进度各自可以用并发多进程

7. 大规模
   可以支持10k节点升级，升级框架本身不再是性能瓶颈。提供相关性能分析接口，在开发过程中就能自动分析各升级组件和外部接口性能。

8. 升级脚本易用性提升

   统一升级脚本，提供可读的升级SDK(整理常用接口, 隔离内外接口，减少参数)，提高开发效率。

   同时提供升级脚本单独调试功能。

   这是各组件升级开发人员和升级接口人重点要求改善的特性。

   详细设计见: [[[升级脚本特性设计]](https://github.com/NicholasTao/NicholasTao.github.io/issues/8#%E5%8D%87%E7%BA%A7%E8%84%9A%E6%9C%AC%E7%89%B9%E6%80%A7%E8%AE%BE%E8%AE%A1)](#升级脚本特性设计)

### 主要改动点

1. 统一数据收集
2. 细粒度升级
3. 解耦

### 对"一跳升级"和"可控升级"的意义

todo

### 新框架对比

详细见下文“新旧框架对比”章节

> 好的架构逻辑清晰，坏的架构一言难尽
>
> 本文可以详细描述新框架的架构逻辑。但是无法详细说明旧框架的架构逻辑，原因就是旧框架是混乱的无法简单梳理清楚的。
>
> 既然旧框架无法说明清楚，如何保证新框架和旧框架的一致？
>
> 升级框架是一个服务，只要对外接口和对外特性保持一致即可。内部实现的变更是不需要一一对应验证的。

## 新框架设计原理

1. 升级工程是由多个小任务组成的。
2. 升级就是单个任务的执行内容和多个任务的执行顺序组成。
3. 升级是简单的。（代码量15k，核心逻辑"启停、升级rpm"）

## 新框架是怎么样的？



### 模块交互

![升级模块详细流程图](新升级框架外部汇报.assets/升级模块详细流程图.png)

1. 升级前编排生成全局计划书，包含了所有升级任务的内容、执行顺序和串并行信息。
2. 升级时根据外部升级命令截取相应的升级任务生成当前计划书。
3. 通过(upg-server)任务下发器下发当前计划书给对应的(upg-client)任务执行器。
4. 任务执行器根据任务内容调用相应任务脚本。
5. 任务脚本执行完成后，任务执行器上报任务给任务状态接收器，接收器固化结果在数据库中，下发器监控到任务完成后继续执行下一批任务。
6. 循环3-5直到当前计划书中全部任务完成。

详细见下文[[[[模块设计]](https://github.com/NicholasTao/NicholasTao.github.io/issues/8#%E6%A8%A1%E5%9D%97%E8%AE%BE%E8%AE%A1)](https://github.com/NicholasTao/NicholasTao.github.io/issues/8#%E6%A8%A1%E5%9D%97%E8%AE%BE%E8%AE%A1)](#模块设计)章节

与旧框架对比见下文[旧框架图]

### 升级顺序

与旧框架相同, 按照以下顺序

> 分发 -> 检查 -> 管理面升级 -> 网络升级 -> 数据面升级-> 提交

这部分逻辑写在新框架的[[逻辑书]](#逻辑书详细设计)中, 未来可以通过改变逻辑书来修改升级顺序。

### 对外接口/升级脚本

与旧框架保持相同

## 新框架如何完成升级任务？

### 整体流程

[信息收集 -> 依次]

Note新增模块

<!--新增前置信息收集模块-->

### 端到端流程

以最典型的升级大任务--管理面升级为例

![image-20221101163005340](新升级框架外部汇报.assets/image-20221101163005340.png)

1. 接收外部升级命令"管理面升级执行"

2. 根据命令[[任务截取器]](#任务截取器详细设计)从[[[[全局计划书]](https://github.com/NicholasTao/NicholasTao.github.io/issues/8#%E5%85%A8%E5%B1%80%E8%AE%A1%E5%88%92%E4%B9%A6%E8%AF%A6%E7%BB%86%E8%AE%BE%E8%AE%A1)](https://github.com/NicholasTao/NicholasTao.github.io/issues/8#%E5%85%A8%E5%B1%80%E8%AE%A1%E5%88%92%E4%B9%A6%E8%AF%A6%E7%BB%86%E8%AE%BE%E8%AE%A1)](#全局计划书详细设计)截取[[[[当前计划书]](https://github.com/NicholasTao/NicholasTao.github.io/issues/8#%E5%BD%93%E5%89%8D%E8%AE%A1%E5%88%92%E4%B9%A6%E8%AF%A6%E7%BB%86%E8%AE%BE%E8%AE%A1)](https://github.com/NicholasTao/NicholasTao.github.io/issues/8#%E5%BD%93%E5%89%8D%E8%AE%A1%E5%88%92%E4%B9%A6%E8%AF%A6%E7%BB%86%E8%AE%BE%E8%AE%A1)](#当前计划书详细设计)，此时当前计划书的内容为

   ```
   [
   	[节点A关闭第1批组件, 节点B关闭第1批组件, ...],
   	[节点A关闭第2批组件, 节点B关闭第2批组件, ...],
   	...
   	[组件X修改配置, 组件Y修改配置, ...],
   	[节点A升级(XY...)组件rpm, 节点B升级(YZ...)组件rpm...]
   	[节点A开启第1批组件, 节点B开启第1批组件, ...],
   	[节点A开启第2批组件, 节点B开启第2批组件, ...],
   	...
   ]
   # 注意到这里是2维数组, 内层数组表示里面的任务是同一批并行执行的，外部数组表示每批任务是串行执行的
   ```

3. 任务下发器下发批节点任务。首先下发的是"关闭第1批组件"

   ```
   [节点A关闭第1批组件, 节点B关闭第1批组件, ...]
   ```

   既将"节点A关闭第1批组件"下发给节点A，"节点B关闭第1批组件"下发给节点B...

4. 任务执行器接收下发的节点任务。以节点A为例，此时节点任务"节点A关闭第1批组件"的内容是

   ```
   [
   	[节点A关闭nova-api, 节点A关闭swift-ngnix, ...]
   	[节点A关闭nova-console, ... ]
   	...
   ]
   # 注意到这里是2维数组, 内层数组表示里面的任务是同一批并行执行的，外部数组表示每批任务是串行执行的
   ```

5. 任务执行器会按照顺序和串并行情况依次调用相应的任务脚本。

   ```
   # 如节点A关闭nova-api任务
   调用stop_nova_api.py脚本
   ```

6. 任务执行器完成所有任务脚本后，上报任务执行结果给任务状态执行器。

   ```
   # 全部成功时的上报
   {
   	节点A关闭nova-api: done,
   	节点A关闭swift-ngnix: done,
   	节点A关闭nova-console: done,
   	...
   }
   
   # 部分失败时的上报
   {
   	节点A关闭nova-api: done,
   	节点A关闭swift-ngnix: fail,
   	节点A关闭nova-console: undo,
   	...
   }
   # 节点A关闭nova-console: undo表示这个任务还未执行时，前置任务已经失败了
   ```

7. 任务状态执行器接受上报后，直接将消息保存在数据库中

   > 此处是旧框架的一个性能瓶颈，主要是保存前后做了很多数据处理
   >
   > 新框架在此处只有保存动作，可以提升性能

8. 工步3下发任务后，任务下发器会定期查询数据库，是否所有下发任务已经完成。当查询到所有任务执行成功后，则继续下发下一批节点任务（存在失败则中断升级）。重复工步3-8，直到当前计划书中所有任务执行完成。

查询: 升级过程中前端需要查询升级进度时，查询器会直接从数据库查询，从而与升级执行逻辑解耦。详细见[[查询器详细设计]](#查询器详细设计)

### 端到端流程时序图

![image-20221103164418952](新升级框架外部汇报.assets/image-20221103164418952.png)

## 新旧框架对比

# 详细设计

## 特性设计

### 灵活升级特性设计

升级工程是由一个个小粒度的任务按照顺序组成的。只要做到任意单任务的升级回退，就可以完成任意组合的升级回退。

场景二:	A节点升级后调测失败

		旧框架:


​			

场景一:

	现象：现网升级中，某个组件在某个节点数据割接出现了问题，导致组件启动后异常。需要先恢复环境，再规避继续升级。
	
	旧框架: 
	
		方法1: 整体回退，整个环境回退到升级前。再修改配置转换逻辑代码，再次从头走升级后逻辑。时间太长，现网基本不会采用。
	
		预期规避时间: 10小时以上
	
		方法2: 人工处理割接后的数据，将数据逐一改成正确数据。规避时间长，人工操作易出错。
	
		预期规避时间: 30分钟                 
	
	新框架：
	
		单独回退该组件该节点的数据割接任务。修改配置转换逻辑代码。重试数据割接任务。
	
		由于只需要回退重试单个小任务，规避时间缩短。
	
		第一次修改配置转换逻辑代码时间30分钟，规避时间5分钟
	
		以后


​		

###  并行升级特性设计

升级(或回退)过程中，随时可以添加新组件，进行并行升级。

> 这里框架只提供底层支持，具体是否可以并行还要看任务或组件之间的依赖关系。

新框架中，升级的数据分为静态和动态数据两种。静态数据如升级信息、部署信息，这部分信息是升级前收集的，升级中保持不变，所以没有并发问题。动态数据是任务状态，由于划分了细粒度任务，新添加组件的升级任务与旧组件的升级任务是分开的所以互不影响。

todo 图：A工程，B工程共享数据，分别状态

应用场景：

	A组件升级过程中，创建新工程升级B组件。参考公有云并行升级。

### 多版本升级特性设计

新框架的任务的粒度是基于instance的，所以每个instance都可以指定是否升级，升级的目标版本是什么。

如下文场景1，实现方法

1. 升级前通过外部命令获取需要升级hostos的计算节点列表，称为待升节点。
2. 升级编排第一步(即生成所有升级任务, 详见[[编排详细设计]](#编排详细设计))时，只生成待升节点的hostos升级任务。
3. 正常编排。此时生成的全局任务书里，已经没有了其它节点的hostos升级任务。
4. 按照全局计划书，执行升级工程。只升级待升节点的hostos升级任务。

应用场景:

1. 控制节点和计算节点的hostos版本是r8，用户想将某几个计算节点升级到r9，其它节点保留在r8以节省升级时间

2. 同一组件两个版本归一

3. 同一组件一个版本分化为两个

### N-X升级特性设计

支持N-X升级的升级框架支持

#### 提供管理面备区升级能力

管理组件随hostos在备区升级后重启生效；同时保留管理面重启单独组件升级方式

#### 其它支持

待补充

### 大规模特性设计

#### 减少任务下发前后的数据处理

新框架中任务在升级前生成并保存在数据库中

```
# 数据库中的任务
h1_nova-api_stop: done
h2_swift-ngnix_stop: done
...
hn_cps-client_uprpm: undo
...

# 当前计划书中的任务
[
	[[[h1_stop_nova-api], [h1_stop_swift-store]], [[h2_stop_nova-api], [h2_stop_swift-store]]],
	...
]
```

下发任务时，只需要

#### 减少任务上报前后的数据处理

每个任务都有一个唯一id, 如"h1_stop_nova-api"。详见[[附录-主要数据结构-任务(Task)]](#任务(Task))

执行器(upg-client)执行完节点任务后，会将子任务的结果上报给状态接收器

```
# 上报结构体
{
	h1_stop_nova-api: done,
	h1_stop_swift-store: fail,
	...
}
```

状态接收器每隔1s钟将接收的数据批量存在数据库中, 详见[[任务状态表设计]](#任务状态表设计)

```
# 保存数据库状态
h1_stop_nova-api: done
h1_stop_swift-store: fail
...
```

以10000节点同时执行任务为例, 每个节点每分钟上报一次。状态接收器相当于每秒执行一次写操作, 修改167条数据。远远小于数据库和python WSGI-server的性能。

由于状态接收器是解耦的

> 这里也可以使用如下设计
>
> 执行器(upg-client)直接访问数据库，执行写-任务状态操作。

